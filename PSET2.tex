%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter this block of commands.  If you're proficient at LaTeX, you may include additional packages, create macros, etc. immediately below this block of commands, but make sure to NOT alter the header, margin, and comment settings here. 
\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, enumitem, fancyhdr, color, comment, graphicx, environ}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\pagestyle{fancy}
\setlength{\headheight}{65pt}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{sol}
    {\emph{Solution:}
    }
    {
    \qed
    }
\specialcomment{com}{ \color{blue} \textbf{Comment:} }{\color{black}} %for instructor comments while grading
\NewEnviron{probscore}{\marginpar{ \color{blue} \tiny Problem Score: \BODY \color{black} }}

\newcounter{subproblem}
% \renewcommand{\thesubproblem}{\alph{subproblem}} % letters 
\renewcommand{\thesubproblem}{\arabic{subproblem}} % numbers
\newenvironment{subprob}[1][]{
  \refstepcounter{subproblem}
  \begin{trivlist}
  \item[\hskip \labelsep {\bfseries (\thesubproblem)}]
}{
  \end{trivlist}
}
\newenvironment{subsol}
    {\emph{Solution:}
    }
    {
    \qed
    }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbff{Z}}
\newcommand{\st}{\text{s.t}}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!10},
  frame=single,
  breaklines=true,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  language=Matlab
}

\setlength {\marginparwidth }{2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\lhead{Class: ECON 20210}
\chead{Assignment: 2}
% \lhead{Anthony Yoon}  %replace with your name
\rhead{Anthony Yoon\\ Min Seo Kim \\ Sam Konkel \\ Pratyush Sharma} %replace XYZ with the homework course number, semester (e.g. ``Spring 2019"), and assignment number.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter this block.
\begin{document}
\listoftodos
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{1}
    Explaining lower real interest rates. 
\end{problem}
\begin{subprob}
\end{subprob}
\begin{subsol}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{fredgraph.png}
        \caption{Fred Graph}
        \label{fig:enter-labe1l}
    \end{figure}
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    \todo{Fill in here}
\end{subsol}
\begin{subprob}
    
\end{subprob}
\begin{subsol}
    We can see that $r = \frac{\partial Y}{\partial K_t} - \delta$. We can compute this as the following:
    \[
    \frac{\partial Y}{\partial K_t} = \alpha K_t^{\alpha - 1} (A_t N_t)^{1-\alpha} = \alpha k^{\alpha - 1}
    \]
    In steady state, note that $\Delta k_t = 0$. This implies that
    \begin{align*}
        \Delta k_t &= s k_t^\alpha - (n + g + \delta)k_t\\
        0 &= sk_t^\alpha - (n + g + \delta)k_t\\
        \frac{n + g + \delta}{s} &= k_t^{\alpha - 1}\\
        \alpha \frac{n + g + \delta}{s} &= \alpha k_t^{\alpha - 1}
    \end{align*}
    Thus, 
    \[
    r = \alpha \frac{n + g + \delta}{s} - \delta
    \]
\end{subsol}

\setcounter{subproblem}{0}
\begin{problem}{2}
    Transitional Dynamics in Solow Growth Model
\end{problem}
\begin{subprob}

\end{subprob}
\begin{subsol}
    Given the given parameters, we can see that:
    \[
k_t = \frac{K_t}{A_t N_t} \quad y_t = \frac{y_t}{A_tN_t} 
\]
Thus
\begin{align*}
    K_{t+1} &= K_t(1-\delta) + I_t\\
    K_{t+1} &= K_t(1 - \delta) + s(A_t N_t)^{1-\alpha} K_t^\alpha\\
    K_{t+1} - K_t &=  s(A_t N_t)^{1-\alpha} K_t^\alpha - \delta K_t\\
    \frac{\Delta K_t}{K_t} &= \frac{s(A_t N_t)^{1-\alpha} K_t^\alpha}{K_t} - \delta \\
    \frac{\Delta K_t}{K_t} &= \frac{s K_t^{\alpha - 1}}{(A_t N_t)^{\alpha -1}} - \delta \\
    \frac{\Delta K_t }{K_t} &= s k_t^{\alpha -1} - \delta\\
\end{align*}
Note that $K_t = A_t N_t k_t$ This implies:
\begin{align*}
    \frac{\Delta K_t }{K_t} &= s k_t^{\alpha -1} - \delta\\
    \frac{\Delta k_t A_t N_t}{k_t A_t N_t} &= s k^{\alpha - 1}_t - \delta \\
    \frac{\Delta k_t}{k_t} + \frac{\Delta A_t}{A_t} + \frac{\Delta N_t}{N_t} &= sk_t^{\alpha - 1} - \delta\\
    \frac{\Delta k_t}{k_t} + g + n &= sk_t^{\alpha - 1} - \delta\\
    \Delta k_t &= sk_t^\alpha - (n + g + \delta) k_t
\end{align*}
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    At steady state, $\Delta k_t = 0$ For notational state, let $x = k_{ss}$. This implies that
    \begin{align*}
        0 &= sx^\alpha - (n+g+\delta)k_t\\
        (n+g+\delta)x &= sx^\alpha\\
        \frac{n+g+\delta}{s} &= x^{\alpha - 1}\\
        x &= \left( \frac{s}{n+g+\delta} \right)^\frac{1}{1-\alpha} 
    \end{align*}
\end{subsol}
\begin{subprob}
    
\end{subprob}
\begin{subsol}
    We are interested in the following optimization problem:
    \begin{align*}
        \max & \quad k_t^\alpha - (n+g+\delta)k_t
    \end{align*}
    Taking the first order deriative with respect to $k_t$ allows to see:
    \[
    \alpha k_t^{\alpha - 1} - (n+g+\delta) = 0 \implies k_{gr} = \left( \frac{\alpha}{n+g+\delta} \right)^\frac{1}{\alpha -1}
    \]
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    Code for the simulation:
    \begin{lstlisting}
 % Setting parameters
s = 0.4;
delta = 0.06;
n = 0.02;
g = 0.02;
alpha = 1/3;
z = 100; % Number of iterations

% x axis creation
X = 0:1:z;
X = X';

K = zeros(z+1, 1);
A = zeros(z+1, 1);
N = zeros(z+1, 1);
k = zeros(z+1,1);
y = zeros(z+1,1);
Y = zeros(z+1,1);

% setting values
A(1) = 1;
K(1) = 1;
N(1) = 1;
Y(1) = K(1)^alpha * (A(1) * N(1))^(1-alpha);

% Time iteration 
for i = 1:(z+1)
    A(i + 1) = A(i) * (1 + g);
    N(i + 1) = N(i) * (1 + n);
    K(i + 1) = K(i) * (1 - delta) + s * (A(i) * N(i))^(1 - alpha) * K(i)^alpha;
    k(i) = (K(i) / (A(i) * N(i)));
    Y(i) = K(i)^alpha * (A(i) * N(i))^(1-alpha);
    y(i) = Y(i) / (A(i) * N(i));
end

figure;

subplot(2, 2, 1);
plot(X, y);
title('Plot of y vs X');
grid on;

subplot(2, 2, 2);
plot(X, Y);
title('Plot of Y vs X');
grid on;

subplot(2, 2, 3);
plot(X, k);
title('Plot of k vs X');
grid on;

subplot(2, 2, 4);
plot(X, K(1:101));
title('Plot of K vs X');
grid on;
    \end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{ECON20210P3Q5.pdf}
    \caption{Figure Econ 20210 Problem 3 Question 5}
    \label{fig:enter-label}
\end{figure}
Note that $k$ and $y$ are appraoch the steady state behaviors and $Y$ and $K$ approach infinity, which resemble the Inada conditions. 
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    \begin{lstlisting}
% Problem 3 Q5

% Setting values 
k_steady_state = (s / (n+g+delta))^(1.5);
k_1 = zeros(z, 1);
k_1(1) = k_steady_state;
s = 0.35;

for i=1:z
    k_1(i+1) = s * k_1(i)^alpha - (n + g + delta) * k_1(i) + k_1(i);
end

figure;

plot(X,k_1)
    \end{lstlisting}
    \textbf{INSERT GRAPH}
\end{subsol}

\setcounter{subproblem}{0}
\todo{REDO}
\begin{problem}{3}
    Cookie Eating - Part 1
\end{problem}
\begin{subprob}
\end{subprob}
\begin{subsol}
    We can see the law of depreciation is:
    \[
    W_{t+1} = W_t - c_t \quad \st \quad W_0 > 0
    \]
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    Note that $W_{t+1} =  W_t - c_t$ and thus $W_t = W_{t-1} - c_{t-1}$. This implies that via a recursive arguement:
    \[
    W_{t+1} = W_t - c_t \implies W_{t+1} = W_0 - \sum_{t = 1}^{T} c_t 
    \]
    such that $W_{t+1} \geq 0 $
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    The Langrangian is as follows:
    \[
    L = s - \lambda \left( W_{t+1} - W_0 + \sum_{0}^{t} c_t \right)
    \]
    with the following FOCs:
    \begin{align*}
        [c_i] & \quad \left(\frac{\partial u}{\partial c} \bigg|_{c_i}\right) \cdot \beta^i + \lambda \leq 0\\
        [\lambda] & \quad W_{t+1} \leq W_0 - \sum_{t = 1}^{T} c_t 
    \end{align*}
    Note that $W_{t+1}$ has to be 0, as no utility is derived from $W_{t+1}$ period. 
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    From a $[c_{i+1}]$ and $[c_i]$, we see that:
    \[
    u'(c_{t+1})  = \frac{\lambda}{\beta^{t+1}} \quad u'(c_t) = \frac{\lambda}{\beta^t}
    \]
    This implies
    \[
    \frac{u'(c_{t})}{u'(c_{t+1})} = \frac{\frac{\lambda}{\beta^t}}{\frac{\lambda}{\beta^{t+1}}} = \beta \iff u'(c_t) = \beta u'(c_{t+1})
    \]
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    From (4), we see that 
    \[
    \beta c_t = c_{t+1} \iff \frac{c_t}{\beta} = c_{t-1}
    \]
    This implies that using the $[\lambda]$ condition, we are innterested in solving:
    \[
    W_0 = \sum_{i = 0}^{t} \beta^{-i} c_t 
    \]
    which is equivalent to 
    \[
    c_t \sum_{i=0}^{t} = W_0 \iff c_t = \frac{W}{\sum_{i=0}^{t} \beta^{-i}}
    \]
\end{subsol}
\todo{Do final part}
\setcounter{subproblem}{0}
\begin{problem}{4}
    Crusoeâ€™s Intratemporal Choice
\end{problem}
\begin{subprob}
\end{subprob}
\begin{subsol}
\begin{align*}
    \max & U(c, l)\\
    \st & \quad c = \frac{1}{1-\theta} (l - \overline{l})^{1-\theta}\\
    \st & \quad 0 < \theta < 1
\end{align*}
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    The FOCs are as follows:
    \begin{align*}
        [c] & \quad \frac{\alpha}{c} - \lambda = 0\\
        [l] & \quad \frac{1-\alpha}{1-l} - \lambda(l - \overline{l})^{-\theta} = 0\\
        [\lambda] & \quad c = \frac{1}{1-\theta} (l - \overline{l})^{1-\theta} 
    \end{align*}
    We see that using the $[l]$ and $[\lambda]$ constraint, we see that
    \begin{align*}
        -\frac{1-\alpha}{1-l} + \frac{a}{c} (l -\overline{l})^{-\theta} &= 0\\
        \frac{\alpha}{c} (l - \overline{l})^{-\theta} &= \frac{1-\alpha}{1-l}\\
        \frac{\alpha}{c(l-\overline{l})^\theta} &= \frac{1-\alpha}{1-l}
    \end{align*}
    Note that $(l - \overline{l})^{\theta}$ is the weight of trade off between the consumption and labor. If $\theta$ increases, working more does become as beneifical. 
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    Using the derived optimality condition, we can see the following:
    \begin{align*}
        \frac{1-\alpha}{1-l} &= \frac{\alpha}{c} (l - \overline{l})^{-\theta}\\
        c(1-\alpha) &= \alpha (1- l )(l - \overline{l})^{-\theta}\\
        c &= \frac{\alpha (1-l)(l- \overline{l})^{-\theta}}{1-\alpha}\\
        \frac{1}{1-\theta} (l - \overline{l})^{1-\theta} &= \frac{\alpha (1-l)(l- \overline{l})^{-\theta}}{1-\alpha}\\
        \frac{l - \overline{l}}{1-\theta} &= \frac{\alpha (1-l)}{1-\alpha}\\
        \frac{l}{1-\theta} + \frac{\alpha l}{1-\alpha} &= \frac{\alpha}{1-\alpha} + \frac{\overline{l}}{1-\theta}\\
        l \left( \frac{1}{1-\theta} + \frac{\alpha}{1-\alpha} \right) &= \frac{\alpha (1-\theta) + \overline{l} (1-\alpha)}{(1-\alpha)(1-\theta)}\\
        l \left( \frac{1-\alpha + \alpha(1-\theta)}{(1-\theta)}(1-\alpha) \right) &= \frac{\alpha (1-\theta) + \overline{l} (1-\alpha)}{(1-\alpha)(1-\theta)}\\
        l &= \frac{\alpha (1-\theta) + \overline{l} (1 - \alpha)}{1-\alpha + \alpha(1-\theta)}
    \end{align*}
    This implies that:
    \begin{align*}
        c &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}(1 - \alpha)}{1 - \alpha + \alpha(1 - \theta)} - \overline{l} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}(1 - \alpha)}{1 - \alpha + \alpha - \alpha\theta} - \overline{l} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}(1 - \alpha)}{1 - \alpha\theta} - \overline{l} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}(1 - \alpha) - \overline{l}(1 - \alpha\theta)}{1 - \alpha\theta} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}[(1 - \alpha) - (1 - \alpha\theta)]}{1 - \alpha\theta} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \overline{l}(\alpha\theta - \alpha)}{1 - \alpha\theta} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta) + \alpha(\theta - 1)\overline{l}}{1 - \alpha\theta} \right)^{1 - \theta} \\
          &= \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta)(1 - \overline{l})}{1 - \alpha\theta} \right)^{1 - \theta}
        \end{align*}
        Therefore:
        \[
           l = \frac{\alpha (1-\theta) + \overline{l} (1 - \alpha)}{1-\alpha + \alpha(1-\theta)} \quad c = \frac{1}{1 - \theta} \left( \frac{\alpha(1 - \theta)(1 - \overline{l})}{1 - \alpha\theta} \right)^{1 - \theta}
        \]        
\end{subsol}
\begin{subprob}
\end{subprob}
\begin{subsol}
    \[
    \frac{\partial l}{\partial \overline{l}} = \frac{1-\alpha}{1-\alpha + \alpha(1-\theta)} \quad 
    \frac{\partial c}{\partial \overline{l}} = -\left( \frac{\alpha(1 - \theta)}{1 - \alpha\theta} \right)^{1 - \theta} (1 - \overline{l})^{-\theta}
    \]
\end{subsol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter anything below this line.
\end{document}