%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter this block of commands.  If you're proficient at LaTeX, you may include additional packages, create macros, etc. immediately below this block of commands, but make sure to NOT alter the header, margin, and comment settings here. 
\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, enumitem, fancyhdr, color, comment, graphicx, environ}
\usepackage{float}
\pagestyle{fancy}
\setlength{\headheight}{65pt}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{sol}
    {\emph{Solution:}
    }
    {
    \qed
    }
\specialcomment{com}{ \color{blue} \textbf{Comment:} }{\color{black}} %for instructor comments while grading
\NewEnviron{probscore}{\marginpar{ \color{blue} \tiny Problem Score: \BODY \color{black} }}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbff{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\st}{\text{s.t}}
\newcommand{\proj}[2]{\mathrm{proj}_{{#2}}\,{#1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\lhead{Class: STAT 24310}
\chead{Assignment: 1}
% \lhead{Anthony Yoon}  %replace with your name
\rhead{Anthony Yoon} %replace XYZ with the homework course number, semester (e.g. ``Spring 2019"), and assignment number.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter this block.
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Copy the following block of text for each problem in the assignment.
\begin{problem}{1}
    
\end{problem}
\begin{sol}
    Note that $f$ is continous at every point in $\R^3$. This implies that Jacobian exists. Let $f_1: \R^3 \to \R, f_1(x_1, x_2, x_3) =  x_1 x_2 + \sin(x_3) + x_1^2$ and $f_2: \R^3 \to \R^1, f_2(x_1, x_2, x_3) = 7 + e^{x_2}$. Therefore 
    \[
    \nabla f_1  = \begin{bmatrix}
        x_2 + 2x_1 & x_2 & \cos(x_3)
    \end{bmatrix} \quad 
    \nabla f_2 = \begin{bmatrix}
        0 & e^{x_2} & 0
    \end{bmatrix}
    \]  
    This implies that 
    \[
    J_x = \begin{bmatrix}
        x_2 + 2x_1 & x_1 & \cos(x_3) \\
        0 & e^{x_2} & 0 
    \end{bmatrix}
    \]
    We now aim to show what induced one norm on a matrix. For any $x \in \R^n$ and $A \in \R^{m \times n}$, we can see that:
    \begin{align*}
        Ax &= \sum_{j = 1}^n a_{ij} x_j\\
        \|Ax\|_1 &= \sum_{i = 1}^{m} \left | \sum_{j = 1}^{n} a_{ij} x_j \right |\\
        &\leq \sum_{i = 1}^{m} \sum_{j = 1}^{n} |a_{ij}| \cdot |x_j|\\
        &\leq \sum_{j = 1}^{n} |x_j| \sum_{i = 1}^{m} |a_{ij}|\\
        &\leq \sum_{j = 1}^{n} |x_j| \max_j |c_j|\\
        &\leq \max_j |c_j|
    \end{align*}
    where $c_j$ denotes the sum of the $j$th column. To prove the reverse direction, we can see that if we let $x = e_j$, where it is the maximum column sum, we can see that 
    \[
    \|Ax\|_1 = \sup_{\|x\|_ 1 = 1} \|Ax\|_1 \geq \max_j |c_j |
    \]  
    which implies that $|A\|_1 = \max_j |c_j|$. Therefore, we see that 
    \[
        k_{abs} = \max \{ |x_2 + 2x_1| , |x_1 + e^{x_2}|, |\cos(x_3)|\}
    \]
    Therefore, since $k_{rel} = k_{abs} \cdot \frac{\|x\|_1}{\|f(x)\|_1}$, we see that:
    \[
    k_{abs} = \max \{ |x_2 + 2x_1| , |x_1 + e^{x_2}|, |\cos(x_3)|\} \cdot \frac{|x_1| + |x_2| + |x_3|}{|x_1 x_2 + \sin(x_3) + x_1^2 + 7 + e^{x_2}|}
    \]
\end{sol}

\newpage
\begin{problem}{2}
    
\end{problem}
\begin{sol}
    Let $x, X, y, Y \in \R$, the following are derived from the statements given.
    \begin{align*}
        x \| \cdot \|_c \leq &\| \cdot \|_a \leq X \| \cdot \|_c\\
        y \| \cdot \|_b \leq &\| \cdot \|_c \leq Y \| \cdot \|_b
    \end{align*}
    We can combine these inequalities to find that:
    \[
    xy \| \cdot \|_b \leq x \| \cdot \|_c \leq \| \cdot \|_a \leq X \| \cdot \|_c \leq  XY \|\cdot\|_b
    \]
    Thus, showing that  $\| \cdot \|_a$ and $\| \cdot \|_b$ are indeed equivalent. 
\end{sol}
\newpage
\begin{problem}{3}
    
\end{problem}
\begin{sol}
    If $p = q$, the statement is obvious. We aim to prove that the $1$ norm is equivalent to the $p$ norm and use transitivity of norms to prove the statement. First, consider $p, q \in \R, p > q \geq 1$. We can consider the following for any $x \in \R^n$ (or $\C^n$):
    \[
    \|x \|_q^q = \sum_{i}^n |x_i|^q = \sum_{i =1}^{n} 1 \cdot |x_i|^q
    \]
    By holder's inequality, we know that:
    \[
    \sum_{i=1}^{n} 1 \cdot (x_i)^q \leq \left( \sum_{i = 1}^{n} 1^a \right)^\frac{1}{a} \left( \sum_{i = 1}^{n} (|x_i|^q)^b \right)^\frac{1}{b}
    \]
    such that $\frac{1}{a} + \frac{1}{b} = 1$. Since the choice of $a$ and $b$ are arbitary, we can let $b$ a value such that $b^{-1} = \frac{q}{p}$. From here, we can perform the following algebra to see that:
    \[
    a = \frac{b }{b-1} = \frac{p}{p-q}
    \]
    and thus:
    \begin{align*}
        \|x\|_q^q & \leq \left( \sum_{i = 1}^{n} 1^a \right)^\frac{1}{a} \left( \sum_{i = 1}^{n} (|x_i|^q)^b \right)^\frac{1}{b}\\
        \|x\|_q^q &\leq \left( \sum_{i = 1}^{n} 1 \right)^\frac{p-q}{p} \left( \sum_{i = 1}^{n} |x_i|^p \right)^\frac{q}{p}\\
        \|x\|_q^q  & \leq n^\frac{p-q}{q} \| x\|_p^q\\
        \|x\|_q & \leq n^\frac{p-q}{pq} \|x\|_p
    \end{align*}
    Let $q = 1$ and $p > 1$. we know aim to bound $\|x\|_1$ from below. Let $\{e_1, e_2, \dots, e_n\}$ denote the standard basis vectors of $\R^n$. Let $x = (x_1, x_2, \dots, x_n)$. and we can see that:
    \[
    x = \sum_{i=1}^{n} e_i x_i
    \]
    Thus, we can see that:
    \[
    \|x\|_p = \left\| \sum_{i=1}^{n} x_i e_i \right \|_p 
    \]
    By the triangle inequality and the properties of the norm, we know that:
    \[
        \left\| \sum_{i=1}^{n} x_i e_i \right \|_p \leq \sum_{i=1}^{n} \|x_i e_i\|_p = \sum_{i=1}^{n} |x_i| \cdot \|e_i\|_p
    \]
    Let $M = \max_{i \in 1,2,\dots, n} \|e_i\|_p$ Thus, it follows that:
    \[
    \|x\|_p \leq \sum_{i=1}^{n}  |x_i| \cdot \|e_i\|_p \leq M \sum_{i=1}^{n} |x_i| = M \|x\|_1
    \]
    Thus, we can see that:
    \[
    \frac{1}{M} \|x\|_p \leq \|x\|_1 \leq n^\frac{p-1}{p} \|x\|_p
    \]
    Thus, we have proved that $1$ norm and the $p$ norm is equivalnet. Thus, by transitive of norms, we can see that $1$ and $q$ are equivalent, which implies that $p$ and $q$ are equivalent norms.
\end{sol}
\newpage
\begin{problem}{4}
    
\end{problem}
\begin{sol}
    Conisder the following: We know that by the definition of the induced norm that:
    \[
    \|Ax\|_a \leq \| A\|_{a \leftarrow c} \cdot \|x\|_c
    \]
    as 
    \[
    \|A\|_{a \leftarrow c} := \sup \frac{\|Ax\|_a}{\|x\|_c}, \forall x \in \R^n
    \]
    Let $y = Bx$, we see that:
    \[
    \|Ay\|_a \leq \|A\|_{a\leftarrow c} \cdot  \|Bx\|_c
    \]
    But, since we know that:
    \[
    \|B\|_{c \leftarrow b} := \sup \frac{\|Bx\|_c}{\|x\|_b}, \forall x \in \R^n
    \]
    we can see that:
    \[
        \|Bx\|_c \leq \| B\|_{c \leftarrow b} \cdot \|x\|_b
    \]
    Thus, we can see that, if we were to combine these two inequalities, we get that:
    \[
        \|Ay\|_{a} \leq \|A\|_{a\leftarrow c} \cdot \|\cdot \| B\|_{c \leftarrow b} \cdot \|x\|_b
    \]
    We can see that 
    \begin{align*}
        \|ABx\|_{a} & \leq \|A\|_{a\leftarrow c} \cdot \|\cdot \| B\|_{c \leftarrow b} \cdot \|x\|_b\\
        \frac{\|ABx\|_{a}}{\|x\|_b} & \leq \|A\|_{a\leftarrow c} \cdot \|\cdot \| B\|_{c \leftarrow b}
    \end{align*}
    We can take the supremum of $\frac{\|ABx\|_{a}}{\|x\|_b}$, and we can see that:
    \[
    \|AB\|_{a \leftarrow b} \leq \|A\|_{a \leftarrow c} \|B\|_{c \leftarrow b}
    \]
\end{sol}
\newpage
\begin{problem}{5}
    
\end{problem}
\begin{sol}
    Consider the subspace of $V_1$ and $V_2$ spanned by the basis $\{ \cos (n \pi x), \sin (\sin (n \pi x))\}$. We can $f \in \{ \cos (n \pi x), \sin (\sin (n \pi x))\}$ is a linear combination, denoted as follows:
    \[
    f = a_n \sin(n \pi x) b_n + \cos (n \pi x)
    \]
    Thus, 
    \[
    \frac{d}{dx} f = n \pi (a_n \cos(n \pi x)  - b_n \sin (n \pi x))
    \]
    Thus, we can see that:
    \[
    T_nf = \begin{bmatrix}
        0 & n \pi \\
        - n \pi 0 
    \end{bmatrix} \begin{bmatrix}
        a_n \sin (n \pi x) \\ b_n \cos( n \pi x)
    \end{bmatrix} = \begin{bmatrix}
        (a_n \cos(n \pi x) \\ - b_n \sin (n \pi x))
    \end{bmatrix}
    \]
    Which implies that
    \[
    T_n = \begin{bmatrix}
        0 & n \pi \\
        - n \pi & 0 
    \end{bmatrix}
    \]
    Which implies that the $T$ is:
    \[
    T = \begin{bmatrix}
        T_1 & 0 & 0 & \dots \\
        0 & T_2 & 0 & \dots \\
        \vdots & \vdots & \ddots & \dots \\
        0 & 0 & 0 & T_n
    \end{bmatrix}
    \]
    where $T_n$ is defined as above. Note that any function in $V_1$ can be defined as follows:
    \[
    f = \sum_{i = 1}^{k} a_n \sin( n \pi x) + b_n \cos ( n \pi x)
    \]
    We now compute $\langle f , f \rangle$. 
    \begin{align*}
        \langle f , f \rangle &= \int_{0}^{1} \left( \sum_{i=1}^{k} a_n \sin(n \pi x) + b_n \cos (n \pi x) \right)^2\\
    \end{align*}
    \[
        = \int_{0}^{1} \sum_{n, m}^k a_n a_m \sin(n \pi x) \sin(m \pi x) + b_n b_m \cos (n \pi x) \cos( m \pi x) + 2a_n b_m \sin(n \pi x) \cos(n \pi x)
    \]
    Note that:
    \[
    \int_{0}^{1} \sin(n \pi x) \sin(m \pi x) = \begin{cases}
        0.5 & n = m \\
        0 & n \neq m
    \end{cases}
    \]
    \[
    \int_{0}^{1} \cos(n \pi x) \cos(m \pi x) = \begin{cases}
        0.5 & n = m \\
        0 & n \neq m
    \end{cases}
    \]
    \[
    \int_{0}^{1} \sin(n \pi x) \cos( n \pi x) = 0, \forall n, m
    \]
    Thus, this implies that:
    \[
    \langle f , f \rangle = \frac{1}{2} \sum_{i = 1}^{k} (a_i^2 + b_i^2)
    \]
    By a similar logic, we can find that:
    \[
    \langle Tf, Tf \rangle = \frac{1}{2} \sum_{n=1}^{k} (n\pi)^2 (a_n^2 + b_n^2)
    \]
    Therefore, we can see that:
    \[
    \left( \frac{\|Tf\|}{\|f\|} \right)^2 = \frac{\sum_{n=1}^{k} (n^2 \pi^2) (a_n^2 + b_n^2)}{\sum_{n=1}^{k} (a_n^2 + b_n^2)}
    \]
    Note that $\sup_f \left( \frac{\|Tf \|}{\|f\|} \right) = k \pi$, where this maximum is obtained when $a_n, b_n = 0$ when $n \neq k$, and $a_k, b_k = 1$, as this is the sum of all weighted averages, and the maximum is achived when we put all the weight when $n = k$. This implies that $\|T \| = k\pi$. We first also wish to prove the existance of a $T^{-1}$. Note that the deriative is a bjiective function, which implies that $T^{-1}$ exists. In this case, we can see that $T^{-1}$ can be defined as follows:
    \[
    T_n^{-1} = \begin{bmatrix}
        0 & \frac{-1}{n \pi} \\
        \frac{1}{n \pi} & 0
    \end{bmatrix}
    \]
    where:
    \[
    T^{-1} = \begin{bmatrix}
        T_1^{-1} & 0 & 0 & \dots \\
        0 & T_2^{-1} & 0 & \dots \\
        \vdots & \vdots & \ddots & \dots \\
        0 & 0 & 0 & T_n^{-1}
    \end{bmatrix}
    \]
    Let $Tf = g$, where $g, f \in V_1$. We can see that:
    \[
    \sup_g \left( \frac{\|T^{-1}g\|}{\|g\|} \right)^2 = \sup \left( \frac{\|f\|}{\|Tf\|} \right)^2 = \sup_n \left( \frac{1}{(n\pi)^2} \right) = \frac{1}{\pi^2}
    \]
    Thus, this implies that $\|T^{-1}\| = \frac{1}{\pi}$ Thus, we see that:
    \[
    \max k_{rel} = \|T\| \|T^{-1}\| = k
    \]
    Thius, this implies that $k_{rel} = O(k)$, as the bounded function as follows:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{MOVETHISPNG.png}
        \caption{Showing that the relative condition is $O(k)$}
        \label{fig:enter-label}
    \end{figure}

\end{sol}
\newpage
\begin{problem}{6}
    
\end{problem}
\begin{sol}
    Let $a_1 = 2\sin (\pi x) + \sin(\pi x)$ and $a_2 = -3 \sin(\pi x) + \sin(2 \pi x)$. Let $q_1 = \frac{a_1}{\|a_1\|}$. Thus, we can proceed with the folloiwng computation:
    \begin{align*}
        \langle a_1, a_1 \rangle &= \langle  2\sin (\pi x) + \sin(\pi x),  2\sin (\pi x) + \sin(\pi x)\rangle \\
        &= \int_{0}^{1} (2\sin(2\pi x) + \sin(\pi x))^2 dx\\
        &= \int_{0}^{1} 4\sin^2(2\pi x) + 4\sin(\pi x) \sin(2\pi x) + \sin^2(\pi x)dx\\
    \end{align*}
    Note that $\int_{0}^{1} \sin(\pi x) \sin( 2\pi x) dx = 0$. Thus, we can see that:
    \begin{align*}
        \langle a_1, a_1 \rangle &= \int_{0}^{1} 4\sin^2(2\pi x) + 4\sin(\pi x) \sin(2\pi x) + \sin^2(\pi x)dx \\
        &= \int_{0}^{1} 4\sin^2(2\pi x) + \sin^2(\pi x)dx \\
    \end{align*}
    Note $\int_{0}^{1} \sin^2( n \pi x) = 0.5, \forall n \in \N$. Thus, we can see that:
    \[
        \langle a_1, a_1 \rangle = \int_{0}^{1} 4\sin^2(2\pi x) + \sin^2(\pi x)dx = 2 + 0.5 = \frac{5}{2}
    \]
    This implies that:
    \[
    q_1 = \sqrt{\frac{2}{5}} (2\sin(2\pi x) + \sin(\pi x))
    \]
    How, we must consider solving the following $v_2 = a_2 - \proj{q_1}{a_2} = a_2 - \langle a_2, q_1 \rangle q_1$. We now the following the integral;
    \begin{align*}
        \sqrt{\frac{2}{5}} \int_{0}^{1} (2\sin(2\pi x) + \sin(\pi x))(\sin(2\pi x) - 3\sin(\pi x))dx &= \sqrt{\frac{2}{5}} \int_{0}^{1} 2\sin^2(2\pi x) - 3\sin^2(\pi x)dx\\
        &= \sqrt{\frac{2}{5}} * \frac{-1}{2}\\
        &= - \sqrt{\frac{1}{10}}
    \end{align*}
    Thus, we see that $v_2 = a_2 - \proj{q_1}{a_2} = a_2 - \langle a_2, q_1 \rangle q_1 = a_2 + \left( \sqrt{\frac{1}{10}} \right) \left( \sqrt{\frac{2}{5}} \right)a_1 = a_2 + 0.2a_1$. Simplifying the vectors, we can see that we can simplify to:
    \[
    -3 \sin (\pi x) + \sin(2\pi x) + 0.2 (2 \sin(2\pi x) + \sin(\pi x)) = \frac{-14}{5} \sin(\pi x) + \frac{7}{5} \sin(2\pi x)
    \] 
    We now proceed with the following calcuation:
    \begin{align*}
        \int_{0}^{1} \left( \frac{7}{5} (-2\sin(\pi x) + \sin(2 \pi x)) \right)^2 dx &= \frac{49}{25} \int_{0}^{1} 4\sin^2 (\pi x) + \sin^2(2\pi x) dx \\
        &=  \frac{49}{25} \left( 2 + \frac{1}{2} \right)\\
        &= \frac{49}{10}
    \end{align*}
    We see that $\| v_2\| = \frac{7}{\sqrt{10}}$. Therefore, we can see that 
    \[
    q_1 = \sqrt{\frac{2}{5}} (2\sin (2\pi x) + \sin(\pi x)), q_2 = \frac{\sqrt{10}}{5} \left( \sin(2 \pi x) - 2\sin(\pi x) \right)
    \]
    Note that $r_{11} = \sqrt{\frac{5}{2}}$, $r_{12} = \frac{\langle a_2, q_1 \rangle}{\|q_1\|} = -\frac{1}{\sqrt{10}}, r_{22} = \frac{7}{\sqrt{10}} = \|v_2\|$. Thus, this implies that:
    \[
    R = \begin{bmatrix}
        \sqrt{\frac{5}{2}} & -\sqrt{\frac{1}{10}} \\
        0 & \frac{7}{\sqrt{10}}
    \end{bmatrix}
    \]
\end{sol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Do not alter anything below this line.
\end{document}